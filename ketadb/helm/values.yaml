
ketadb:
  clusterName: "ketadb"
  replicas: 3
  podManagementPolicy: Parallel  # 并发部署
  updateStrategy: RollingUpdate  # 滚动升级

  image: 
    repository: "docker.ketaops.cc/ketaops/ketadb"
    tag: "v1.2.2-apps"
    PullPolicy: "Always"

  pod:
    labels: {}  # 为pod添加标签
    annotations: {}  # 为pod添加注释

  service:
    labels: {}
    type: ClusterIP
    nodePort: ""
    annotations: {}
    ports:
      http: "9200"
      transport: "9300"
      monitor: "10005"
      heartbeat: "9400"
      rpc: "9500"
      debug: "9600"
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    extraPorts: []

  resources:
    requests:
      cpu: "100m"
      memory: "2Gi"
    limits:
      cpu: "4"
      memory: "8Gi"

  persistence:
    enabled: true
    annotations: {}

  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    # storageClassName: ""
    resources:
      requests:
        storage: 10Gi

  podSecurityContext:
    fsGroup: 0
    runAsUser: 0

  securityContext:
    privileged: true #  Processes in privileged containers are essentially equivalent to root on the host.
    capabilities: 
      drop: []
      add:
        - SYS_ADMIN
        - NET_RAW
        - NET_ADMIN
    # readOnlyRootFilesystem: true
    runAsNonRoot: false
    runAsUser: 0

  ingress:
    enabled: false
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: "1024m"
      kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    path: /
    hostsSuffix:
      - example.com
      
  nodeSelector: {}
  tolerations: []
  # Hard means that by default pods will only be scheduled if there are enough nodes for them
  # and that they will never end up on the same node. Setting this to soft will do this "best effort"
  antiAffinity: "soft"
  nodeAffinity: {}

  # This is the PriorityClass settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ""

  # How long to wait for ketadb to stop gracefully
  terminationGracePeriod: 30
  
  sysctlInitContainer:
    enabled: true
    sysctlVmMaxMapCount: 262144
    resources: {}

  extraInitContainers: []

  readinessProbe:
    failureThreshold: 10
    initialDelaySeconds: 30
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 10

  livenessProbe:
    failureThreshold: 10
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 10

  dcServerUrl: ""
  dcServerHeartbeats: ""
  dcDataTransUrls: ""
  alertHost: ""

  licenseCheckInterval: "300"

  unicast: ""
  roles:
    master: true
    web: true
    data: true
    ingest: 
  
  env:
    

  envFrom: [] 
  # - secretRef:
  #     name: env-secret
  # - configMapRef:
  #     name: config-map

  secretMounts: []
  #  - name: elastic-certificates
  #    secretName: elastic-certificates
  #    path: /usr/share/ketadb/config/certs

  extraVolumeMounts: []
  # - name: extras
  #   mountPath: /usr/share/extras
  #   readOnly: true

  extraContainers: []
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

